Sign Language Recognition Model
This project is a sign language recognition model that uses computer vision techniques to recognize sign language gestures in real-time.

Overview
The model is built using the MediaPipe framework for hand detection and tracking, and a custom deep learning model for gesture recognition. It uses a dataset of sign language gestures to train the model, and achieves an accuracy of 86% on the test set.

Requirements
Python 3.x
OpenCV
MediaPipe
TensorFlow
NumPy

Installation
Clone the repository:
```git clone https://github.com/mithz-z/Sign-Language-Recognition-Using-LSTM-in-Streamlit.git```

Install the required packages:
```pip install -r requirements.txt```

Run the application:
```python Streamlit.py```

Usage
Run the Streamlit.py file to start the application.
Use the webcam to capture your hand gestures.
The model will recognize the gestures in real-time and display the result on the screen.

License
This project is licensed under the MIT License - see the LICENSE file for details.
